{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZzv73_UWb6N"
   },
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/divider.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bagging..png)\n",
    "\n",
    "\n",
    "Now we are about to embark on our journey from simple decision trees to algorithms that use decision trees as components. The path goes like this:\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/dbxg.png)\n",
    "\n",
    "The use of decision trees began in the 1980s and XGBoost was introduced in 2016. Throughout the next few notebooks we will explore this progression of algorithms.  \n",
    "\n",
    "### A collective of classifiers\n",
    "\n",
    "To gain an intuition on how this works, let's look at how our confidence might increase when more people tell us something. Whether it is multiple doctors giving us the same diagnosis or something as simple as ...\n",
    "\n",
    "#### The Mary Spender example\n",
    "\n",
    "Let's say one of your friends mentions over lunch that you would love a particular musical artist on YouTube, say Mary Spender, who you never heard before. \n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/MarySpender2.png)\n",
    "\n",
    "What is the chance that you will actually like Mary Spender's music? Maybe slighly better than chance? Let's say you think there is a 60% chance you will like her. You will file away the recommendation but you are not going to rush home and watch a YouTube video.  Now, in addition to the lunch friend's recommendation,  an old music school friend, now living in Austin messages you saying you should check out Mary Spender and the friend predicts you will absolutely love her. Then a week later, while talking with an old bandmate over the phone, that bandmate, again, recommends Mary Spender. Over the course of less than 10 days, three of your friends independently (because they don't know one another) recommend Mary Spender. Now what is the likelihood of you liking Mary Spender? I am guessing you think that now it is higher than 60%. Maybe now you think it is 90% likely you will like her. It is the aggregate of these 3 people's opinions (3 classifiers) that ups the accuracy of the prediction.\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/spender22.png)\n",
    "\n",
    "\n",
    "This is similar to how bagging works. One aggregates the votes of a number of classifiers and the vote of that ensemble of classifiers is more accurate than that of a single classifier. Even if the accuracy of each component classifier is low (known as a weak classifier), the ensemble can be a strong (high accuracy) classifier. Of course there are some caveats. \n",
    "\n",
    "Back to the Mary Spender example. Suppose one of your friends went to a Mary Spender concert and then later in the week met with four other of your friends and mentioned that she thought you would love Mary Spender's music. Then, over the course of a week all those friends recommended Mary Spender to you. In this case the recommendations are not that independent---all are based on one person's opinion. Thus, the accuracy would not be as great as in the example above. Similarly, if you made 10 copies of the exact same classifier each trained on exactly the same data, the accuracy of the ensemble of clones would not be any better than the accuracy of a single copy. Moving away from Mary Spender and our musical tastes and back to machine learning, we can try to create independence among the classifier in 2 ways:\n",
    "\n",
    "1. We can change the type of classifier. For example, we can use a k-Nearest Neighbor Classifier with Manhattan distance and a k of 5, a k-Nearest Neighbor Classifier with Euclidean distance and a k of 3, a decision tree classifier using entropy and a max depth of 5, and a decision tree classifier with using gini and no max depth specified. Hopefully, the accuracy of the ensemble of the four classifiers would be greater than that of a single classifier.\n",
    "2. We can have an ensemble of the same classifier (for example, 10 decision tree classifiers with identical hyperparameters) but each classifier can get a different subset of the training data. The classifiers would thus build different models (differents 'rules') and, again, the accuracy of the ensemble should be greather than that of a single classifier. This is the approach we will take.\n",
    "\n",
    "### Bagging and Pasting\n",
    "\n",
    "\n",
    "In this Jupyter notebook, we are going to explore Bagging algorithms. Bagging algorithms come in a variety of 'flavors' including one called 'bagging' and one called 'pasting'.\n",
    "\n",
    "But first an experiment on the what *with replacement* means. As you will see shortly, that term is the crucial difference between bagging and pasting.\n",
    "\n",
    "### A small experiment\n",
    "NOTE: The following code is just used for illustration and is nothing we will be using for machine learning. \n",
    "\n",
    "Consider a list of 5 red balls and 5 blue balls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xkhfXZatWb6S"
   },
   "outputs": [],
   "source": [
    "bag = ['red', 'red', 'red', 'red', 'red',\n",
    "       'blue', 'blue', 'blue', 'blue', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugElXnm3Wb6T"
   },
   "source": [
    "Suppose we want to pick 7 random balls from this list. Python offers two functions that will give us random elements from a list.One is called `choices` which selects a sample with replacement, which means that once a ball is selected it is put back in the bag so it has the potential to be selected again. Let's give it a try, and just because things are random let's do this 100 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6f_BpdyWb6T",
    "outputId": "b606c6f4-564d-4775-800b-a76dda4b416d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 blue and 1 red\n",
      "1 blue and 6 red\n",
      "6 blue and 1 red\n",
      "6 blue and 1 red\n",
      "1 blue and 6 red\n",
      "6 blue and 1 red\n",
      "1 blue and 6 red\n",
      "6 blue and 1 red\n",
      "Balls selected exceeded balls in bag: 8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    set = random.choices(bag, k=7)\n",
    "    blue = set.count('blue')\n",
    "    red = set.count('red')\n",
    "    if blue > 5 or red > 5:\n",
    "        print(\"%i blue and %i red\" % (set.count('blue'), set.count('red')))\n",
    "        total +=1\n",
    "print(\"Balls selected exceeded balls in bag: %i\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxWdBvoGWb6U"
   },
   "source": [
    "*A reminder: please don't mindlessly execute the code. Look at it and understand it*\n",
    "\n",
    "There are five blue balls. Since we are doing the selection with replacement there are times when we select more than 5 blue balls (or five red ones).  \n",
    "\n",
    "When we print\n",
    "\n",
    "```\n",
    "Balls selected exceeded balls in bag: 13\n",
    "```\n",
    "\n",
    "it shows how many times that was the case.\n",
    "\n",
    "\n",
    "When I ran this, 14 times out of 100 had more of one color ball than there were in the original bag. In fact, several times I ended up with all 7 of the balls blue, even though the original list had only 5 balls:\n",
    "\n",
    "```\n",
    "7 blue and 0 red\n",
    "6 blue and 1 red\n",
    "7 blue and 0 red\n",
    "1 blue and 6 red\n",
    "0 blue and 7 red\n",
    "1 blue and 6 red\n",
    "1 blue and 6 red\n",
    "6 blue and 1 red\n",
    "6 blue and 1 red\n",
    "1 blue and 6 red\n",
    "6 blue and 1 red\n",
    "1 blue and 6 red\n",
    "6 blue and 1 red\n",
    "6 blue and 1 red\n",
    "Balls selected exceeded balls in bag: 14\n",
    "```\n",
    "Again, this is called selecting with replacement (we put what we selected back in the set before selecting again). \n",
    "\n",
    "The other alternative is to select without replacement--once we select something we can't select it again. Python's `sample` does this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wjTz87jbWb6U",
    "outputId": "3d08fb12-8262-4b56-82bd-bb230863a611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balls selected exceeded balls in bag: 0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    set = random.sample(bag, k=7)\n",
    "    blue = set.count('blue')\n",
    "    red = set.count('red')\n",
    "    if blue > 5 or red > 5:\n",
    "        print(\"%i blue and %i red\" % (set.count('blue'), set.count('red')))\n",
    "        total +=1\n",
    "print(\"Balls selected exceeded balls in bag: %i\" % (total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-RBRpyMWb6U"
   },
   "source": [
    "As you can see, the number of a specific colored ball that we select never exceeded the number of balls of that color in the original set.\n",
    "\n",
    "Now back to bagging and pasting. In both approaches we are going to sample the training data. Let's say we want 70% of the training data in our sample. In bagging ([Breiman, 1996](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf)), if we our training dataset is 1000 instances and we want 70% for a particular classifier, the algorithm will randomly select 700 out of the 1,000 **with replacement**. With pasting ([Breiman, 1998](https://link.springer.com/article/10.1023/A:1007563306331)), the selection is done **without replacement**. \n",
    "\n",
    "#### but wait, there is more ...\n",
    "\n",
    "There are two other options. Instead of selecting a random subset of training data instances, we can select a random subset of columns (features). Let's say we have a dataset of 1,000 instances each with 100 features. When we select a random subset of columns, we still have 1,000 instances but now they have just a subset of the features. This is called Random Subspaces ([Ho, 1998](https://pdfs.semanticscholar.org/b41d/0fa5fdaadd47fc882d3db04277d03fb21832.pdf?_ga=2.196949164.1638238666.1596910000-1073138517.1596910000)).\n",
    "\n",
    "Finally, we can train a classifier on both random subsets of instances and random subsets of features. This is known as Random Patches ([Louppe and Geurts, 2012](https://www.researchgate.net/publication/262212941_Ensembles_on_Random_Patches))\n",
    "\n",
    "In summary, the four methods are:\n",
    "\n",
    "* **bagging** - select a subset of data set instances using replacement\n",
    "* **pasting** - select a subset of data set instances without replacement\n",
    "* **Random Subspaces** - select a subset of features\n",
    "* **Random Patches** - select both a subset of features and of instances\n",
    "\n",
    "Let's see how this works!\n",
    "\n",
    "First, let's grab the Wisconsin Cancer data we used before:\n",
    "\n",
    "#### Wisconsin Cancer Dataset\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/aimam.png)\n",
    "image from Nvidia's [AI Improves Breast Cancer Diagnoses by Factoring Out False Positives](https://blogs.nvidia.com/blog/2018/02/01/making-mammography-more-meaningful/)\n",
    "\n",
    "[A description of the Cancer Database](#Breast-Cancer-Database)\n",
    "\n",
    "In this dataset we are trying to predict the diagnosis---either M (malignant) or B (benign).\n",
    "\n",
    "Let's load the dataset and split it into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "WMWJytA6Wb6W",
    "outputId": "555a3cba-d467-48ed-f3b9-01bc528e33b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radiusAvg</th>\n",
       "      <th>textureAvg</th>\n",
       "      <th>perimeterAvg</th>\n",
       "      <th>areaAvg</th>\n",
       "      <th>smoothnessAvg</th>\n",
       "      <th>compactnessAvg</th>\n",
       "      <th>concavityAvg</th>\n",
       "      <th>concavityPointsAvg</th>\n",
       "      <th>symmetryAvg</th>\n",
       "      <th>...</th>\n",
       "      <th>radiusWorst</th>\n",
       "      <th>textureWorst</th>\n",
       "      <th>perimeterWorst</th>\n",
       "      <th>areaWorst</th>\n",
       "      <th>smoothnessWorst</th>\n",
       "      <th>compactnessWorst</th>\n",
       "      <th>concavityWorst</th>\n",
       "      <th>concavityPointsWorst</th>\n",
       "      <th>symmetryWorst&gt;</th>\n",
       "      <th>FractalDimensionWorst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>912558</th>\n",
       "      <td>B</td>\n",
       "      <td>13.700</td>\n",
       "      <td>17.64</td>\n",
       "      <td>87.76</td>\n",
       "      <td>571.1</td>\n",
       "      <td>0.09950</td>\n",
       "      <td>0.07957</td>\n",
       "      <td>0.04548</td>\n",
       "      <td>0.03160</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>...</td>\n",
       "      <td>14.960</td>\n",
       "      <td>23.53</td>\n",
       "      <td>95.78</td>\n",
       "      <td>686.5</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.1346</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.09077</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>0.06960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877500</th>\n",
       "      <td>M</td>\n",
       "      <td>14.450</td>\n",
       "      <td>20.22</td>\n",
       "      <td>94.49</td>\n",
       "      <td>642.7</td>\n",
       "      <td>0.09872</td>\n",
       "      <td>0.12060</td>\n",
       "      <td>0.11800</td>\n",
       "      <td>0.05980</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>...</td>\n",
       "      <td>18.330</td>\n",
       "      <td>30.12</td>\n",
       "      <td>117.90</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>0.15520</td>\n",
       "      <td>0.4056</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.18380</td>\n",
       "      <td>0.4753</td>\n",
       "      <td>0.10130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90602302</th>\n",
       "      <td>M</td>\n",
       "      <td>15.500</td>\n",
       "      <td>21.08</td>\n",
       "      <td>102.90</td>\n",
       "      <td>803.1</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.15710</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.08481</td>\n",
       "      <td>0.2085</td>\n",
       "      <td>...</td>\n",
       "      <td>23.170</td>\n",
       "      <td>27.65</td>\n",
       "      <td>157.10</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.4002</td>\n",
       "      <td>0.4211</td>\n",
       "      <td>0.21340</td>\n",
       "      <td>0.3003</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899667</th>\n",
       "      <td>M</td>\n",
       "      <td>15.750</td>\n",
       "      <td>19.22</td>\n",
       "      <td>107.10</td>\n",
       "      <td>758.6</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0.23640</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>...</td>\n",
       "      <td>17.360</td>\n",
       "      <td>24.17</td>\n",
       "      <td>119.40</td>\n",
       "      <td>915.3</td>\n",
       "      <td>0.15500</td>\n",
       "      <td>0.5046</td>\n",
       "      <td>0.6872</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.10500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90401602</th>\n",
       "      <td>B</td>\n",
       "      <td>12.800</td>\n",
       "      <td>17.46</td>\n",
       "      <td>83.05</td>\n",
       "      <td>508.3</td>\n",
       "      <td>0.08044</td>\n",
       "      <td>0.08895</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.04083</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>...</td>\n",
       "      <td>13.740</td>\n",
       "      <td>21.06</td>\n",
       "      <td>90.72</td>\n",
       "      <td>591.0</td>\n",
       "      <td>0.09534</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.08296</td>\n",
       "      <td>0.1988</td>\n",
       "      <td>0.07053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866674</th>\n",
       "      <td>M</td>\n",
       "      <td>19.790</td>\n",
       "      <td>25.12</td>\n",
       "      <td>130.40</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.15890</td>\n",
       "      <td>0.25450</td>\n",
       "      <td>0.11490</td>\n",
       "      <td>0.2202</td>\n",
       "      <td>...</td>\n",
       "      <td>22.630</td>\n",
       "      <td>33.58</td>\n",
       "      <td>148.70</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.3861</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>0.17320</td>\n",
       "      <td>0.3305</td>\n",
       "      <td>0.08465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914769</th>\n",
       "      <td>M</td>\n",
       "      <td>18.490</td>\n",
       "      <td>17.52</td>\n",
       "      <td>121.30</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>0.10120</td>\n",
       "      <td>0.13170</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.09183</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>...</td>\n",
       "      <td>22.750</td>\n",
       "      <td>22.88</td>\n",
       "      <td>146.40</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.14120</td>\n",
       "      <td>0.3089</td>\n",
       "      <td>0.3533</td>\n",
       "      <td>0.16630</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.09445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895100</th>\n",
       "      <td>M</td>\n",
       "      <td>20.340</td>\n",
       "      <td>21.51</td>\n",
       "      <td>135.90</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.25650</td>\n",
       "      <td>0.15040</td>\n",
       "      <td>0.2569</td>\n",
       "      <td>...</td>\n",
       "      <td>25.300</td>\n",
       "      <td>31.86</td>\n",
       "      <td>171.10</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>0.15920</td>\n",
       "      <td>0.4492</td>\n",
       "      <td>0.5344</td>\n",
       "      <td>0.26850</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>0.10240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859711</th>\n",
       "      <td>B</td>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.2436</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914333</th>\n",
       "      <td>B</td>\n",
       "      <td>14.870</td>\n",
       "      <td>20.21</td>\n",
       "      <td>96.12</td>\n",
       "      <td>680.9</td>\n",
       "      <td>0.09587</td>\n",
       "      <td>0.08345</td>\n",
       "      <td>0.06824</td>\n",
       "      <td>0.04951</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>16.010</td>\n",
       "      <td>28.48</td>\n",
       "      <td>103.90</td>\n",
       "      <td>783.6</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         diagnosis  radiusAvg  textureAvg  perimeterAvg  areaAvg  \\\n",
       "id                                                                 \n",
       "912558           B     13.700       17.64         87.76    571.1   \n",
       "877500           M     14.450       20.22         94.49    642.7   \n",
       "90602302         M     15.500       21.08        102.90    803.1   \n",
       "899667           M     15.750       19.22        107.10    758.6   \n",
       "90401602         B     12.800       17.46         83.05    508.3   \n",
       "...            ...        ...         ...           ...      ...   \n",
       "866674           M     19.790       25.12        130.40   1192.0   \n",
       "914769           M     18.490       17.52        121.30   1068.0   \n",
       "895100           M     20.340       21.51        135.90   1264.0   \n",
       "859711           B      8.888       14.64         58.79    244.0   \n",
       "914333           B     14.870       20.21         96.12    680.9   \n",
       "\n",
       "          smoothnessAvg  compactnessAvg  concavityAvg  concavityPointsAvg  \\\n",
       "id                                                                          \n",
       "912558          0.09950         0.07957       0.04548             0.03160   \n",
       "877500          0.09872         0.12060       0.11800             0.05980   \n",
       "90602302        0.11200         0.15710       0.15220             0.08481   \n",
       "899667          0.12430         0.23640       0.29140             0.12420   \n",
       "90401602        0.08044         0.08895       0.07390             0.04083   \n",
       "...                 ...             ...           ...                 ...   \n",
       "866674          0.10150         0.15890       0.25450             0.11490   \n",
       "914769          0.10120         0.13170       0.14910             0.09183   \n",
       "895100          0.11700         0.18750       0.25650             0.15040   \n",
       "859711          0.09783         0.15310       0.08606             0.02872   \n",
       "914333          0.09587         0.08345       0.06824             0.04951   \n",
       "\n",
       "          symmetryAvg  ...  radiusWorst  textureWorst  perimeterWorst  \\\n",
       "id                     ...                                              \n",
       "912558         0.1732  ...       14.960         23.53           95.78   \n",
       "877500         0.1950  ...       18.330         30.12          117.90   \n",
       "90602302       0.2085  ...       23.170         27.65          157.10   \n",
       "899667         0.2375  ...       17.360         24.17          119.40   \n",
       "90401602       0.1574  ...       13.740         21.06           90.72   \n",
       "...               ...  ...          ...           ...             ...   \n",
       "866674         0.2202  ...       22.630         33.58          148.70   \n",
       "914769         0.1832  ...       22.750         22.88          146.40   \n",
       "895100         0.2569  ...       25.300         31.86          171.10   \n",
       "859711         0.1902  ...        9.733         15.67           62.56   \n",
       "914333         0.1487  ...       16.010         28.48          103.90   \n",
       "\n",
       "          areaWorst  smoothnessWorst  compactnessWorst  concavityWorst  \\\n",
       "id                                                                       \n",
       "912558        686.5          0.11990            0.1346          0.1742   \n",
       "877500       1044.0          0.15520            0.4056          0.4967   \n",
       "90602302     1748.0          0.15170            0.4002          0.4211   \n",
       "899667        915.3          0.15500            0.5046          0.6872   \n",
       "90401602      591.0          0.09534            0.1812          0.1901   \n",
       "...             ...              ...               ...             ...   \n",
       "866674       1589.0          0.12750            0.3861          0.5673   \n",
       "914769       1600.0          0.14120            0.3089          0.3533   \n",
       "895100       1938.0          0.15920            0.4492          0.5344   \n",
       "859711        284.4          0.12070            0.2436          0.1434   \n",
       "914333        783.6          0.12160            0.1388          0.1700   \n",
       "\n",
       "          concavityPointsWorst  symmetryWorst>  FractalDimensionWorst  \n",
       "id                                                                     \n",
       "912558                 0.09077          0.2518                0.06960  \n",
       "877500                 0.18380          0.4753                0.10130  \n",
       "90602302               0.21340          0.3003                0.10480  \n",
       "899667                 0.21350          0.4245                0.10500  \n",
       "90401602               0.08296          0.1988                0.07053  \n",
       "...                        ...             ...                    ...  \n",
       "866674                 0.17320          0.3305                0.08465  \n",
       "914769                 0.16630          0.2510                0.09445  \n",
       "895100                 0.26850          0.5558                0.10240  \n",
       "859711                 0.04786          0.2254                0.10840  \n",
       "914333                 0.10170          0.2369                0.06599  \n",
       "\n",
       "[114 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "colNames = ['id', 'diagnosis', 'radiusAvg', 'textureAvg', 'perimeterAvg', 'areaAvg',\n",
    "            'smoothnessAvg', 'compactnessAvg', 'concavityAvg', 'concavityPointsAvg',\n",
    "            'symmetryAvg', 'FractalDimensionAvg', 'radiusSE', 'textureSE', 'perimeterSE',\n",
    "            'areaSE','smoothnessSE', 'compactnessSE', 'concavitySE', 'concavityPointsSE',\n",
    "            'symmetrySE', 'FractalDimensionSE', 'radiusWorst', 'textureWorst', 'perimeterWorst',\n",
    "            'areaWorst', 'smoothnessWorst', 'compactnessWorst', 'concavityWorst', 'concavityPointsWorst',\n",
    "            'symmetryWorst>', 'FractalDimensionWorst']\n",
    "len(colNames)\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/wdbc.data', names=colNames)\n",
    "data.set_index('id', inplace=True)\n",
    "\n",
    "trainingdata, testdata = train_test_split(data, test_size = 0.2)\n",
    "testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEQMMnQEWb6b"
   },
   "source": [
    "Now divide up the data into the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BUFu_lp1Wb6b"
   },
   "outputs": [],
   "source": [
    "colNames.remove('id')\n",
    "colNames.remove('diagnosis')\n",
    "trainingDataFeatures = trainingdata[colNames]\n",
    "testDataFeatures = testdata[colNames]\n",
    "trainingDataLabels = trainingdata['diagnosis']\n",
    "testDataLabels = testdata['diagnosis']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jwhHLytWb6c"
   },
   "source": [
    "Let's get a base accuracy using a single decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0451MpM0Wb6d",
    "outputId": "54502712-b622-4920-d26e-a3220ae12c12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385964912280702"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = clf.predict(testDataFeatures)\n",
    "\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asjigQIyWb6d"
   },
   "source": [
    "Now we will see if we can improve on that accuracy.\n",
    "\n",
    "### Building a bagging classifier\n",
    "\n",
    "Let's build a collective of 20 decision tree classifiers (`n_estimators`). Let's train each one with 100 random samples from our dataset (`max_samples`) with replacement (`bootstrap=True`). `n_jobs` means how many jobs to run in parallel. `n_jobs=-1` means use all available CPU cores.   \n",
    "\n",
    "Just to reinforce the vocabulary we are learning, `n_estimators`, `max_samples`, `bootstrap` are among the **hyperparameters** of the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tL_K6UxpWb6d",
    "outputId": "9875eb5b-6436-4d52-811a-9decb08e2653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "bagging_clf = BaggingClassifier(clf, n_estimators=20, max_samples=100, \n",
    "                                bootstrap=True, n_jobs=-1)\n",
    "bagging_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = bagging_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1XYY6-2Wb6d"
   },
   "source": [
    "When I did this using a single decision tree classifier was 90.3% accurate, while the bagging classifier was 96.5% accurate--halving the error rate! that's pretty good!\n",
    "\n",
    "\n",
    "### Pasting\n",
    "Let's try the same thing with pasting (without replacement):\n",
    "\n",
    "For that we set the hyperparameter: `bootstrap=False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIwO0j1xWb6e",
    "outputId": "051274f2-4b92-4f36-e78a-191c5a90c98e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasting_clf = BaggingClassifier(clf, n_estimators=20, max_samples=100, \n",
    "                                bootstrap=False, n_jobs=-1)\n",
    "pasting_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = pasting_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcCz_Gu_Wb6f"
   },
   "source": [
    "### Random Subspaces\n",
    "Again, random subspaces are when we randomly select feature subsets rather than subsets of the dataset instances. This time we will create 50 classifiers for our collective and each will train on a dataset with 7 features (`max_feature=7`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCf10iICWb6f",
    "outputId": "939ca298-550d-4ea8-f69a-9c20c9dd94b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=50, max_features=7, \n",
    "                                bootstrap=True, n_jobs=-1)\n",
    "subspace_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = subspace_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-9qdIG-Wb6f"
   },
   "source": [
    "### Random Patches\n",
    "Finally, let's combine things and try random patches. In this example each classifier will be given a subset of 100 training instances with 7 features each.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCjPbM08Wb6g",
    "outputId": "c882e0f4-90ed-4abd-ee74-3192c5bd95ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=100, max_features=7, \n",
    "                                 max_samples=100, bootstrap=False, n_jobs=-1)\n",
    "subspace_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = subspace_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bD3S8xDWb6g"
   },
   "source": [
    "While it is common to use a decision tree as the base classifier, we can use any classifier. Here we use kNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuTZAjpIWb6g",
    "outputId": "3560e63b-29e4-4787-d96b-6cf6aa023f41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier()\n",
    "bagging_clf = BaggingClassifier(kNN, n_estimators=20, max_samples=100, \n",
    "                                bootstrap=True, n_jobs=-1)\n",
    "bagging_clf.fit(trainingDataFeatures, trainingDataLabels)\n",
    "predictions = bagging_clf.predict(testDataFeatures)\n",
    "accuracy_score(testDataLabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_av0jdqWb6g"
   },
   "source": [
    "#### Summary\n",
    "As you can see, any of these simple bagging algorithms typically outperforms using a single classifier. \n",
    "\n",
    "\n",
    "### Review\n",
    "\n",
    "We import the bagging classifier library with:\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "```\n",
    "\n",
    "and create an instance of one with:\n",
    "\n",
    "```\n",
    "my_bagging_classifier = BaggingClassifier(baseClassifier, Hyperparameters,n_jobs=-1)\n",
    "```\n",
    "\n",
    "#### Base Classifier\n",
    "while any classifier can be used we typically use a decision tree\n",
    "\n",
    "#### Hyperparameters\n",
    "Here is a list of the hyperparameters (from the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)):\n",
    "\n",
    "* `n_estimators`: integer, default value = 10, the number of classifiers (estimators) in the ensemble.\n",
    "* `max_samples`: integer or float, default value = 1.0(meaning use all the training instances), the number of samples (instances) to draw from the training dataset to train each base classifier.\n",
    "    * if integer, then draw max_features features.\n",
    "    * if float, then draw max_samples * X.shape[0] samples. For example if `max_samples` is 0.7 and there are 100 instances in the training dataset then draw 70 samples.\n",
    "* `max_features`, integer or float, default value =1.0,  \n",
    "the number of features to draw from the training dataset to train each base estimator \n",
    "    * if integers, then draw max_features features.\n",
    "    * if float, then draw max_features * X.shape[1] features.\n",
    "* `bootstrap` boolean, default value =True, whether samples and features are drawn with replacement. If False, sampling without replacement is performed.\n",
    "\n",
    "For other hyperparamters, consult the documentation.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/datamining-guide/master/labs/pics/torchdivide.png)\n",
    "\n",
    "\n",
    "# <font color='#EE4C2C'>You Try ...</font> \n",
    "## <font color='#EE4C2C'>Predicting musical genres from audio file attributes</font> \n",
    "\n",
    "\n",
    "When you listen even to a few seconds of a song you can identify it as blues, country, classical, or any other genre. How do you do this? What attributes are you hearing in the audio file that helps you make this classification? And, more to the point, can we train a computer to do it?\n",
    "\n",
    "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/bluesClassical.png)\n",
    "\n",
    "We are going to be using  the [GTZAN Dataset for Music Genre Classification](https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification). It provides data of 100 songs for each of 10 genres. The data is in several formats:\n",
    "\n",
    "* 30 second audio files (wav)\n",
    "* spectral images of those 30 second clips (see image above)\n",
    "* a csv file containing acoustic attributes of the 30 second clip\n",
    "* a csv file containing acoustic attributes of 3 second clips (the 30 second clips were split into 3 second ones)\n",
    "\n",
    "We are going to use the 3 second csv file which is available at \n",
    "\n",
    "https://raw.githubusercontent.com/zacharski/ml-class/master/data/gtzan.csv\n",
    "\n",
    "Go ahead and load the data into a dataframe (the first row contains feature names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IiF0cRc4Wb6h"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00000.0.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335406</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>1773.065032</td>\n",
       "      <td>167541.630869</td>\n",
       "      <td>1972.744388</td>\n",
       "      <td>117335.771563</td>\n",
       "      <td>...</td>\n",
       "      <td>39.687145</td>\n",
       "      <td>-3.241280</td>\n",
       "      <td>36.488243</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>38.099152</td>\n",
       "      <td>-5.050335</td>\n",
       "      <td>33.618073</td>\n",
       "      <td>-0.243027</td>\n",
       "      <td>43.771767</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00000.1.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.112699</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>1816.693777</td>\n",
       "      <td>90525.690866</td>\n",
       "      <td>2010.051501</td>\n",
       "      <td>65671.875673</td>\n",
       "      <td>...</td>\n",
       "      <td>64.748276</td>\n",
       "      <td>-6.055294</td>\n",
       "      <td>40.677654</td>\n",
       "      <td>0.159015</td>\n",
       "      <td>51.264091</td>\n",
       "      <td>-2.837699</td>\n",
       "      <td>97.030830</td>\n",
       "      <td>5.784063</td>\n",
       "      <td>59.943081</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00000.2.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.346815</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.132003</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1788.539719</td>\n",
       "      <td>111407.437613</td>\n",
       "      <td>2084.565132</td>\n",
       "      <td>75124.921716</td>\n",
       "      <td>...</td>\n",
       "      <td>67.336563</td>\n",
       "      <td>-1.768610</td>\n",
       "      <td>28.348579</td>\n",
       "      <td>2.378768</td>\n",
       "      <td>45.717648</td>\n",
       "      <td>-1.938424</td>\n",
       "      <td>53.050835</td>\n",
       "      <td>2.517375</td>\n",
       "      <td>33.105122</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00000.3.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.086856</td>\n",
       "      <td>0.132565</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1655.289045</td>\n",
       "      <td>111952.284517</td>\n",
       "      <td>1960.039988</td>\n",
       "      <td>82913.639269</td>\n",
       "      <td>...</td>\n",
       "      <td>47.739452</td>\n",
       "      <td>-3.841155</td>\n",
       "      <td>28.337118</td>\n",
       "      <td>1.218588</td>\n",
       "      <td>34.770935</td>\n",
       "      <td>-3.580352</td>\n",
       "      <td>50.836224</td>\n",
       "      <td>3.630866</td>\n",
       "      <td>32.023678</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00000.4.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>1630.656199</td>\n",
       "      <td>79667.267654</td>\n",
       "      <td>1948.503884</td>\n",
       "      <td>60204.020268</td>\n",
       "      <td>...</td>\n",
       "      <td>30.336359</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>45.880913</td>\n",
       "      <td>1.689446</td>\n",
       "      <td>51.363583</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>26.738789</td>\n",
       "      <td>0.536961</td>\n",
       "      <td>29.146694</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>rock.00099.5.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.349126</td>\n",
       "      <td>0.080515</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>1499.083005</td>\n",
       "      <td>164266.886443</td>\n",
       "      <td>1718.707215</td>\n",
       "      <td>85931.574523</td>\n",
       "      <td>...</td>\n",
       "      <td>42.485981</td>\n",
       "      <td>-9.094270</td>\n",
       "      <td>38.326839</td>\n",
       "      <td>-4.246976</td>\n",
       "      <td>31.049839</td>\n",
       "      <td>-5.625813</td>\n",
       "      <td>48.804092</td>\n",
       "      <td>1.818823</td>\n",
       "      <td>38.966969</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>rock.00099.6.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.372564</td>\n",
       "      <td>0.082626</td>\n",
       "      <td>0.057897</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>1847.965128</td>\n",
       "      <td>281054.935973</td>\n",
       "      <td>1906.468492</td>\n",
       "      <td>99727.037054</td>\n",
       "      <td>...</td>\n",
       "      <td>32.415203</td>\n",
       "      <td>-12.375726</td>\n",
       "      <td>66.418587</td>\n",
       "      <td>-3.081278</td>\n",
       "      <td>54.414265</td>\n",
       "      <td>-11.960546</td>\n",
       "      <td>63.452255</td>\n",
       "      <td>0.428857</td>\n",
       "      <td>18.697033</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>rock.00099.7.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.347481</td>\n",
       "      <td>0.089019</td>\n",
       "      <td>0.052403</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>1346.157659</td>\n",
       "      <td>662956.246325</td>\n",
       "      <td>1561.859087</td>\n",
       "      <td>138762.841945</td>\n",
       "      <td>...</td>\n",
       "      <td>78.228149</td>\n",
       "      <td>-2.524483</td>\n",
       "      <td>21.778994</td>\n",
       "      <td>4.809936</td>\n",
       "      <td>25.980829</td>\n",
       "      <td>1.775686</td>\n",
       "      <td>48.582378</td>\n",
       "      <td>-0.299545</td>\n",
       "      <td>41.586990</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>rock.00099.8.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.387527</td>\n",
       "      <td>0.084815</td>\n",
       "      <td>0.066430</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>2084.515327</td>\n",
       "      <td>203891.039161</td>\n",
       "      <td>2018.366254</td>\n",
       "      <td>22860.992562</td>\n",
       "      <td>...</td>\n",
       "      <td>28.323744</td>\n",
       "      <td>-5.363541</td>\n",
       "      <td>17.209942</td>\n",
       "      <td>6.462601</td>\n",
       "      <td>21.442928</td>\n",
       "      <td>2.354765</td>\n",
       "      <td>24.843613</td>\n",
       "      <td>0.675824</td>\n",
       "      <td>12.787750</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>rock.00099.9.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.369293</td>\n",
       "      <td>0.086759</td>\n",
       "      <td>0.050524</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>1634.330126</td>\n",
       "      <td>411429.169769</td>\n",
       "      <td>1867.422378</td>\n",
       "      <td>119722.211518</td>\n",
       "      <td>...</td>\n",
       "      <td>38.801735</td>\n",
       "      <td>-11.598399</td>\n",
       "      <td>58.983097</td>\n",
       "      <td>-0.178517</td>\n",
       "      <td>55.761299</td>\n",
       "      <td>-6.903252</td>\n",
       "      <td>39.485901</td>\n",
       "      <td>-3.412534</td>\n",
       "      <td>31.727489</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9990 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0     blues.00000.0.wav   66149          0.335406         0.091048  0.130405   \n",
       "1     blues.00000.1.wav   66149          0.343065         0.086147  0.112699   \n",
       "2     blues.00000.2.wav   66149          0.346815         0.092243  0.132003   \n",
       "3     blues.00000.3.wav   66149          0.363639         0.086856  0.132565   \n",
       "4     blues.00000.4.wav   66149          0.335579         0.088129  0.143289   \n",
       "...                 ...     ...               ...              ...       ...   \n",
       "9985   rock.00099.5.wav   66149          0.349126         0.080515  0.050019   \n",
       "9986   rock.00099.6.wav   66149          0.372564         0.082626  0.057897   \n",
       "9987   rock.00099.7.wav   66149          0.347481         0.089019  0.052403   \n",
       "9988   rock.00099.8.wav   66149          0.387527         0.084815  0.066430   \n",
       "9989   rock.00099.9.wav   66149          0.369293         0.086759  0.050524   \n",
       "\n",
       "       rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0     0.003521             1773.065032          167541.630869   \n",
       "1     0.001450             1816.693777           90525.690866   \n",
       "2     0.004620             1788.539719          111407.437613   \n",
       "3     0.002448             1655.289045          111952.284517   \n",
       "4     0.001701             1630.656199           79667.267654   \n",
       "...        ...                     ...                    ...   \n",
       "9985  0.000097             1499.083005          164266.886443   \n",
       "9986  0.000088             1847.965128          281054.935973   \n",
       "9987  0.000701             1346.157659          662956.246325   \n",
       "9988  0.000320             2084.515327          203891.039161   \n",
       "9989  0.000067             1634.330126          411429.169769   \n",
       "\n",
       "      spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0                 1972.744388           117335.771563  ...   39.687145   \n",
       "1                 2010.051501            65671.875673  ...   64.748276   \n",
       "2                 2084.565132            75124.921716  ...   67.336563   \n",
       "3                 1960.039988            82913.639269  ...   47.739452   \n",
       "4                 1948.503884            60204.020268  ...   30.336359   \n",
       "...                       ...                     ...  ...         ...   \n",
       "9985              1718.707215            85931.574523  ...   42.485981   \n",
       "9986              1906.468492            99727.037054  ...   32.415203   \n",
       "9987              1561.859087           138762.841945  ...   78.228149   \n",
       "9988              2018.366254            22860.992562  ...   28.323744   \n",
       "9989              1867.422378           119722.211518  ...   38.801735   \n",
       "\n",
       "      mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
       "0       -3.241280   36.488243     0.722209   38.099152    -5.050335   \n",
       "1       -6.055294   40.677654     0.159015   51.264091    -2.837699   \n",
       "2       -1.768610   28.348579     2.378768   45.717648    -1.938424   \n",
       "3       -3.841155   28.337118     1.218588   34.770935    -3.580352   \n",
       "4        0.664582   45.880913     1.689446   51.363583    -3.392489   \n",
       "...           ...         ...          ...         ...          ...   \n",
       "9985    -9.094270   38.326839    -4.246976   31.049839    -5.625813   \n",
       "9986   -12.375726   66.418587    -3.081278   54.414265   -11.960546   \n",
       "9987    -2.524483   21.778994     4.809936   25.980829     1.775686   \n",
       "9988    -5.363541   17.209942     6.462601   21.442928     2.354765   \n",
       "9989   -11.598399   58.983097    -0.178517   55.761299    -6.903252   \n",
       "\n",
       "      mfcc19_var  mfcc20_mean  mfcc20_var  label  \n",
       "0      33.618073    -0.243027   43.771767  blues  \n",
       "1      97.030830     5.784063   59.943081  blues  \n",
       "2      53.050835     2.517375   33.105122  blues  \n",
       "3      50.836224     3.630866   32.023678  blues  \n",
       "4      26.738789     0.536961   29.146694  blues  \n",
       "...          ...          ...         ...    ...  \n",
       "9985   48.804092     1.818823   38.966969   rock  \n",
       "9986   63.452255     0.428857   18.697033   rock  \n",
       "9987   48.582378    -0.299545   41.586990   rock  \n",
       "9988   24.843613     0.675824   12.787750   rock  \n",
       "9989   39.485901    -3.412534   31.727489   rock  \n",
       "\n",
       "[9990 rows x 60 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "music = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/gtzan.csv')\n",
    "music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAAvRO8JWb6h"
   },
   "source": [
    "Let's examine the values of the label column (the genres):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0m3K-MgiWb6h",
    "outputId": "f4565126-207e-4af7-bda8-61f3b098b355"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz',\n",
       "       'metal', 'pop', 'reggae', 'rock'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlKrZfoVWb6h"
   },
   "source": [
    "Those are the 10 genres we are trying to predict. So if we were just to guess without hearing the clip, we would be accurate 10% of the time. How accurate do you think you would be based on hearing a 3 second clip? I am pretty confident I could correctly label the 30 second clips, but I am much less confident about labeling 3 second ones. Since guessing randomly would give me 10% accuracy, I am estimating maybe 50-60% accuracy. Let's see how a computer does.\n",
    "\n",
    "#### Feature Names\n",
    "So the column we are trying to predict is `label`. Now let's get the names of the feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "D1bvL8bCWb6h",
    "outputId": "2b1fa136-b34a-4ff3-ac01-70c822894493",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length', 'chroma_stft_mean', 'chroma_stft_var', 'rms_mean', 'rms_var', 'spectral_centroid_mean', 'spectral_centroid_var', 'spectral_bandwidth_mean', 'spectral_bandwidth_var', 'rolloff_mean', 'rolloff_var', 'zero_crossing_rate_mean', 'zero_crossing_rate_var', 'harmony_mean', 'harmony_var', 'perceptr_mean', 'perceptr_var', 'tempo', 'mfcc1_mean', 'mfcc1_var', 'mfcc2_mean', 'mfcc2_var', 'mfcc3_mean', 'mfcc3_var', 'mfcc4_mean', 'mfcc4_var', 'mfcc5_mean', 'mfcc5_var', 'mfcc6_mean', 'mfcc6_var', 'mfcc7_mean', 'mfcc7_var', 'mfcc8_mean', 'mfcc8_var', 'mfcc9_mean', 'mfcc9_var', 'mfcc10_mean', 'mfcc10_var', 'mfcc11_mean', 'mfcc11_var', 'mfcc12_mean', 'mfcc12_var', 'mfcc13_mean', 'mfcc13_var', 'mfcc14_mean', 'mfcc14_var', 'mfcc15_mean', 'mfcc15_var', 'mfcc16_mean', 'mfcc16_var', 'mfcc17_mean', 'mfcc17_var', 'mfcc18_mean', 'mfcc18_var', 'mfcc19_mean', 'mfcc19_var', 'mfcc20_mean', 'mfcc20_var']\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "featureNames = list(music.columns)\n",
    "featureNames.remove('filename')\n",
    "featureNames.remove('label')\n",
    "print(featureNames)\n",
    "print(len(featureNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-B5g-RAWb6h"
   },
   "source": [
    "So we have 58 features. \n",
    "\n",
    "#### Training and test sets\n",
    "Now it is time to construct the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1zqluOErWb6h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      length  chroma_stft_mean  chroma_stft_var  rms_mean   rms_var  \\\n",
       " 4399   66149          0.413359         0.088487  0.196776  0.011297   \n",
       " 4071   66149          0.455667         0.085775  0.273479  0.007970   \n",
       " 356    66149          0.298642         0.093392  0.181708  0.003126   \n",
       " 49     66149          0.322253         0.086122  0.083275  0.000733   \n",
       " 5653   66149          0.234807         0.085609  0.160609  0.002839   \n",
       " ...      ...               ...              ...       ...       ...   \n",
       " 9225   66149          0.333480         0.081123  0.152629  0.001049   \n",
       " 4859   66149          0.426850         0.088682  0.091590  0.001755   \n",
       " 3264   66149          0.557041         0.076671  0.100618  0.005000   \n",
       " 9845   66149          0.335359         0.086459  0.090398  0.000648   \n",
       " 2732   66149          0.378514         0.087291  0.089495  0.000921   \n",
       " \n",
       "       spectral_centroid_mean  spectral_centroid_var  spectral_bandwidth_mean  \\\n",
       " 4399             2427.083185           1.043407e+06              2826.386534   \n",
       " 4071             2145.875826           4.140569e+05              2357.879178   \n",
       " 356              1745.642113           7.339226e+05              2335.189309   \n",
       " 49               1795.860851           1.685483e+05              1756.318999   \n",
       " 5653             1584.686556           1.110564e+05              1889.770337   \n",
       " ...                      ...                    ...                      ...   \n",
       " 9225             2399.608264           1.285616e+05              2278.406832   \n",
       " 4859             2162.025254           2.288463e+05              2341.549488   \n",
       " 3264             3175.599626           2.867253e+06              3078.772332   \n",
       " 9845             2083.173097           1.837956e+05              1998.118746   \n",
       " 2732             1577.710311           3.846914e+05              1926.991033   \n",
       " \n",
       "       spectral_bandwidth_var  rolloff_mean  ...  mfcc16_mean  mfcc16_var  \\\n",
       " 4399           178010.751162   5433.324069  ...     3.138236   65.920425   \n",
       " 4071           210997.166919   4899.300631  ...     5.795342   55.808830   \n",
       " 356            248007.512152   4132.635780  ...    -7.577754   40.139694   \n",
       " 49              47768.332986   3647.558969  ...     0.062563   48.236279   \n",
       " 5653            80446.769282   2961.974910  ...     2.136964   96.547737   \n",
       " ...                      ...           ...  ...          ...         ...   \n",
       " 9225            34884.271192   5136.331505  ...     4.476577   45.167110   \n",
       " 4859            60409.703202   4952.139799  ...     9.263982   38.472660   \n",
       " 3264           396453.221308   6591.147837  ...    -0.387911   24.767830   \n",
       " 9845            72594.651305   4224.483173  ...    -7.810218   51.331745   \n",
       " 2732           245547.315441   3478.357685  ...    -2.845356   40.332779   \n",
       " \n",
       "       mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
       " 4399    -3.850182   67.284554    -1.009765  100.288109    -4.196325   \n",
       " 4071    -2.640937   42.192661     2.021688   31.996845    -2.548398   \n",
       " 356    -10.012132   34.024105    -6.805904   42.522671   -12.450970   \n",
       " 49      -7.629800   76.484543     0.285192   51.797806    -5.529919   \n",
       " 5653    -4.974571   37.395855    -1.262982   72.164597    -0.593442   \n",
       " ...           ...         ...          ...         ...          ...   \n",
       " 9225   -10.608470   52.106556     7.164659   27.778339    -1.089431   \n",
       " 4859     4.113808   40.541397     4.770267   49.343208     1.426361   \n",
       " 3264    -1.733681   30.956144     0.405525   39.738777    -0.881728   \n",
       " 9845   -20.063759  175.020920     2.085326   44.644463   -11.571122   \n",
       " 2732    -7.791642   34.048908    -1.784111   33.712502   -10.684777   \n",
       " \n",
       "       mfcc19_var  mfcc20_mean  mfcc20_var  \n",
       " 4399  124.731026    -6.965169  110.029945  \n",
       " 4071   32.529030     1.030604   20.973385  \n",
       " 356    40.414761   -15.854045   26.655382  \n",
       " 49     64.883820   -11.223873   35.870258  \n",
       " 5653   42.856293     2.191491  190.096985  \n",
       " ...          ...          ...         ...  \n",
       " 9225   44.320950     6.817255   44.638443  \n",
       " 4859   32.094498     1.973160   40.051491  \n",
       " 3264   27.157696    -3.793831   31.899960  \n",
       " 9845   73.881218     0.414044   82.834877  \n",
       " 2732   23.730682   -11.573023   29.591631  \n",
       " \n",
       " [7992 rows x 58 columns],\n",
       "       length  chroma_stft_mean  chroma_stft_var  rms_mean   rms_var  \\\n",
       " 8474   66149          0.372060         0.091308  0.120653  0.001287   \n",
       " 8857   66149          0.733551         0.015941  0.418318  0.000213   \n",
       " 9686   66149          0.325368         0.089568  0.051838  0.000534   \n",
       " 799    66149          0.472334         0.074339  0.267231  0.002148   \n",
       " 9854   66149          0.440494         0.071545  0.135566  0.000855   \n",
       " ...      ...               ...              ...       ...       ...   \n",
       " 4141   66149          0.519255         0.073104  0.151889  0.008547   \n",
       " 2735   66149          0.350163         0.082854  0.034347  0.000335   \n",
       " 5977   66149          0.291162         0.088058  0.069818  0.000406   \n",
       " 6083   66149          0.527944         0.058112  0.299349  0.002098   \n",
       " 8813   66149          0.476687         0.087766  0.186771  0.009075   \n",
       " \n",
       "       spectral_centroid_mean  spectral_centroid_var  spectral_bandwidth_mean  \\\n",
       " 8474             2863.575280          542673.071796              2596.028644   \n",
       " 8857             5414.708959            7159.380965              3223.250146   \n",
       " 9686             3032.420576          918717.024799              2452.805187   \n",
       " 799              2336.402230          156111.492919              2332.833082   \n",
       " 9854             2521.352339           83211.660919              2017.155021   \n",
       " ...                      ...                    ...                      ...   \n",
       " 4141             2598.721501          199705.329750              2060.634290   \n",
       " 2735              966.179162          141955.208774              1356.939745   \n",
       " 5977             1853.138392          269694.586875              2201.641756   \n",
       " 6083             2407.322541           74269.228833              2459.054359   \n",
       " 8813             2455.523837          880566.205755              2597.498558   \n",
       " \n",
       "       spectral_bandwidth_var  rolloff_mean  ...  mfcc16_mean  mfcc16_var  \\\n",
       " 8474            55705.526777   5962.875225  ...     4.791413   50.712162   \n",
       " 8857             3010.863195   9274.681866  ...    -2.771570    1.553053   \n",
       " 9686            75332.574389   5978.362568  ...     6.433334   61.315834   \n",
       " 799             30868.328578   5106.516301  ...     3.136367   31.954609   \n",
       " 9854            28492.839073   4659.205416  ...     6.682501   24.630066   \n",
       " ...                      ...           ...  ...          ...         ...   \n",
       " 4141            79574.142822   4917.852314  ...     3.463772   91.327522   \n",
       " 2735           176316.705791   1991.572829  ...    -1.580384   67.317810   \n",
       " 5977           100396.609800   4258.770658  ...     6.503087   50.863853   \n",
       " 6083            32544.780903   5335.844914  ...     9.899271   24.026201   \n",
       " 8813           269777.739306   5227.350699  ...     7.730676   56.768608   \n",
       " \n",
       "       mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  \\\n",
       " 8474    -1.865458   57.059864     7.426891   38.414814    -6.095347   \n",
       " 8857    -5.360513    3.853597    -2.058677   14.231124    -3.343481   \n",
       " 9686   -13.197737   34.461765    -1.891385   60.532436    -6.796290   \n",
       " 799    -13.642503   43.941143     4.235998   21.990633    -7.163545   \n",
       " 9854    -6.051273   40.086929     6.421707   41.819939    -7.139108   \n",
       " ...           ...         ...          ...         ...          ...   \n",
       " 4141     1.189089   98.209038    12.539443  117.435333     3.518344   \n",
       " 2735    -6.136338   27.278139     1.832989   48.558998     2.422109   \n",
       " 5977    -4.868603   45.855915     2.954169   58.154823    -5.289443   \n",
       " 6083    -8.465195   25.252417     3.416865   28.036894    -0.934080   \n",
       " 8813     2.224946   70.172684    -3.281772   36.442699     0.166470   \n",
       " \n",
       "       mfcc19_var  mfcc20_mean  mfcc20_var  \n",
       " 8474   53.098068    -4.039930   56.471512  \n",
       " 8857    9.602206    -1.401847    0.282131  \n",
       " 9686   98.297424    -3.002217   52.876480  \n",
       " 799    28.251629    -0.605158   19.771526  \n",
       " 9854   26.480080    -0.982220   27.456194  \n",
       " ...          ...          ...         ...  \n",
       " 4141   83.264198    -0.546494   43.914303  \n",
       " 2735   38.254639    -5.643003   59.845795  \n",
       " 5977   52.718349    -2.480971   50.783180  \n",
       " 6083   14.351903     6.812169   11.726810  \n",
       " 8813   36.352432     2.634236   41.664505  \n",
       " \n",
       " [1998 rows x 58 columns],\n",
       " 4399     hiphop\n",
       " 4071     hiphop\n",
       " 356       blues\n",
       " 49        blues\n",
       " 5653       jazz\n",
       "          ...   \n",
       " 9225       rock\n",
       " 4859     hiphop\n",
       " 3264      disco\n",
       " 9845       rock\n",
       " 2732    country\n",
       " Name: label, Length: 7992, dtype: object,\n",
       " 8474     reggae\n",
       " 8857     reggae\n",
       " 9686       rock\n",
       " 799       blues\n",
       " 9854       rock\n",
       "          ...   \n",
       " 4141     hiphop\n",
       " 2735    country\n",
       " 5977       jazz\n",
       " 6083      metal\n",
       " 8813     reggae\n",
       " Name: label, Length: 1998, dtype: object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## divide the original data 80% going into the music_training dataset \n",
    "## the rest in music_test\n",
    "X = music[featureNames]\n",
    "Y = music[\"label\"]\n",
    "X,Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0)\n",
    "X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "touv2_GAWb6h"
   },
   "source": [
    "### Building a single decision tree classifier\n",
    "Let's build a single decision tree classifier called `clf` using entropy, fit it to the data, make predictions and determine the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1V3O5patWb6h",
    "outputId": "b5933cb6-9227-47f3-fdf2-654a8904a136"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6711711711711712"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEcDPHNUWb6h"
   },
   "source": [
    "When I did this I got 66% accuracy. That doesn't sound great but keep in mind that random guessing would only be 10% accuracy. \n",
    "\n",
    "### Building a Random Patch Classifier\n",
    "\n",
    "Now we are going to build a random patch classifier.\n",
    "\n",
    "* the base classifier will be a decision tree using entropy\n",
    "* the ensemble will contain 20 base classifiers\n",
    "* each classifier will use a random sample of 70% of the training data\n",
    "* each classifier will use a random sample of 70% of the features\n",
    "* the sampling will be done with replacement\n",
    "* it will use all available cpu cores.\n",
    "\n",
    "We are going to\n",
    "\n",
    "1. build the classifier\n",
    "2. train the classifier on the data\n",
    "3. make predictions on the test set\n",
    "4. determine the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HjTNhPVdWb6h",
    "outputId": "3c54e349-d1f7-49b3-f39e-c63ff3b8ec64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348348348348348"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=100, max_features=58, \n",
    "                                 max_samples=3000, bootstrap=False, n_jobs=-1)\n",
    "subspace_clf.fit(X_train, Y_train)\n",
    "predictions = subspace_clf.predict(X_test)\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W3cc2--Wb6h"
   },
   "source": [
    "What accuracy did you get? Was it better than using a single classifier?\n",
    "Keep your original code above. Make a copy of it below and \n",
    "experiment a bit with the hyperparameters. (try 3 or 4 different things) What is the best accuracy you can get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "p83tIBVhWb6i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7652652652652653"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=200, max_features=58, \n",
    "                                 max_samples=1000, bootstrap=True, n_jobs=-1)\n",
    "subspace_clf.fit(X_train, Y_train)\n",
    "predictions = subspace_clf.predict(X_test)\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FkuEN3GUWb6i"
   },
   "outputs": [],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=70, max_features=58, \n",
    "                                 max_samples=2000, bootstrap=True, n_jobs=-1)\n",
    "subspace_clf.fit(X_train, Y_train)\n",
    "predictions = subspace_clf.predict(X_test)\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "FkuEN3GUWb6i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8413413413413413"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subspace_clf = BaggingClassifier(clf, n_estimators=150, max_features=58, \n",
    "                                 max_samples=3000, bootstrap=False, n_jobs=-1)\n",
    "subspace_clf.fit(X_train, Y_train)\n",
    "predictions = subspace_clf.predict(X_test)\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP2mAHhTWb6i"
   },
   "source": [
    "\n",
    "##<font color='#52BE80'>NOTES</font>\n",
    "\n",
    "### Breast Cancer Database\n",
    "\n",
    "[back](#Wisconsin-Cancer-Dataset)\n",
    "\n",
    "  This breast cancer databases was obtained from the University of Wisconsin\n",
    "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
    "   when using this database, then please include this information in your\n",
    "   acknowledgements.  Also, please cite one or more of:\n",
    "\n",
    "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology\", \n",
    "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
    "      December 1990, pp 9193-9196.\n",
    "\n",
    "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
    "      via linear programming: Theory and application to medical diagnosis\", \n",
    "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
    "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
    "\n",
    "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
    "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
    "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
    "\n",
    "Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "\n",
    "Sources:\n",
    "   -- Dr. WIlliam H. Wolberg (physician)\n",
    "      University of Wisconsin Hospitals\n",
    "      Madison, Wisconsin\n",
    "      USA\n",
    "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "      Received by David W. Aha (aha@cs.jhu.edu)\n",
    "   -- Date: 15 July 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4yT3Ci2Wb6i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Working version of bagging_n_pasting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dataenv",
   "language": "python",
   "name": "dataenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
